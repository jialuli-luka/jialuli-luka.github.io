<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jialu Li</title>

  <meta name="author" content="Jialu Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jialu Li</name>
              </p>
              <p>
                <strong>ðŸš¨ I'm on the job market, looking for industry RS positions starting in Summer 2025!</strong>
              </p>
              <p>Hi, thanks for stopping by.
              </p>
              <p>
                I'm a fifth-year Ph.D. student at The University of North Carolina at Chapel Hill, advised by <a href = "http://www.cs.unc.edu/~mbansal/">Prof. Mohit Bansal</a>. Before joining UNC-CH, I got my Master degree from Cornell University, where I was advised by <a href = "http://www.cs.cornell.edu/home/cardie/">Prof. Claire Cardie</a>. I did my Bachelor degree at Shanghai JiaoTong University.
              </p>
              <p style="text-align:center">
                <a href="mailto:jialuli@cs.unc.edu">Email</a> &nbsp/&nbsp
                <a href="data/CV_Jialu.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=KyI1vSgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/jialuli96?lang=en">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/jialuli-luka">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Jialu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Jialu.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have a broad interest in Multimodal research, with a focus on text-to-image generation, Vision-and-Language Navigation, and multi-modal LLM.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <div style="overflow-y: scroll; height: 150px;">
                <style>
                  div::-webkit-scrollbar {
                    width: 3px; /* Scrollbar width */
                  }

                  div::-webkit-scrollbar-track {
                    background: #f1f1f1; /* Track color */
                  }

                  div::-webkit-scrollbar-thumb {
                    background: #888; /* Thumb color */
                  }

                  div::-webkit-scrollbar-thumb:hover {
                    background: #555; /* Thumb color on hover */
                  }
                </style>
              <p>
                <li> We have a paper accepted to <strong>NeurIPS 2024</strong>.</li>
                <li> I will intern at <strong>Google</strong> as Student Researcher for Summer 2024. </li>
                <li> We have a paper accepted to <strong>AAAI 2024</strong>.</li>
                <li> We have a paper accepted to <strong>NeurIPS 2023</strong>.</li>
                <li> We have a paper accepted to <strong>ICCV 2023</strong> and selected as <strong>Oral presentation</strong>. </li>
                <li> I will intern at <strong>Apple</strong> as Machine Learning Research Intern for Summer 2023. </li>
                <li> We have a paper accepted to <strong>CVPR 2023</strong>. </li>
                <li> We have a paper accepted to <strong>Findings of NAACL 2022</strong>. </li>
                <li> We have a paper accepted to <strong>CVPR 2022</strong>. </li>
                <li> I will intern at <strong>Amazon</strong> as Applied Scientist for Summer 2022. </li>
                <li> We have a paper accepted to <strong>EMNLP 2021</strong>. </li>
                <li> We have a paper accepted to <strong>NAACL 2021</strong>. </li>
                <li> We have a paper accepted to <strong>EMNLP 2020</strong>. </li>
                <li> I will join UNC-CH as a new Ph.D. student in Fall 2020. </li>
              </p>
            </div>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <video autoplay controls muted loop width="100%">
              <source src="images/universe.mov" type="video/mp4">
            </video>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Unbounded: A Generative Infinite Game of Character Life Simulation</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://nealwadhwa.com/">Neal Wadhwa</a>, <a href="https://scholar.google.co.il/citations?user=Zi5KiDsAAAAJ&hl=en">Yael Pritch</a>, <a href="https://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, <a href="https://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>.
            <br>
            <em>Preprint</em>
            <br>
            <a href="https://arxiv.org/abs/2410.18975">paper</a> / 
            <a href="https://generative-infinite-game.github.io/">bib</a> /
            <a href="https://generative-infinite-game.github.io/">website</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <video autoplay controls muted loop width="100%">
              <source src="images/dreamrunner.mp4" type="video/mp4">
            </video>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</papertitle>
            <br>
            <a href="https://zunwang1.github.io/">Zun Wang</a>, <strong>Jialu Li</strong>, <a href="https://hl-hanlin.github.io/">Han Lin</a>, <a href="https://jaehong31.github.io/">Jaehong Yoon</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>Preprint</em>
            <br>
            <a href="https://arxiv.org/abs/2411.16657">paper</a> / 
            <a href="https://github.com/wz0919/DreamRunner">code</a> /
            <a href="https://zunwang1.github.io/DreamRunner">bib</a> /
            <a href="https://zunwang1.github.io/DreamRunner">website</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/vln-srdf.png' width="160">
        </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel</papertitle>
            <br>
            <a href="https://zunwang1.github.io/">Zun Wang</a>, <strong>Jialu Li</strong>, <a href="https://yiconghong.me/">Yicong Hong</a>, <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=8rBMUD4AAAAJ">Songze Li</a>, <a href="https://andy1621.github.io/">Kunchang Li</a>, <a href="https://yui010206.github.io/">Shoubin Yu</a>, <a href="https://shepnerd.github.io/">Yi Wang</a>, <a href="https://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>,<a href="https://scholar.google.com/citations?user=hD948dkAAAAJ&hl=zh-CN">Yali Wang</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, <a href="https://wanglimin.github.io/">Limin Wang</a>.
            <br>
            <em>Preprint</em>
            <br>
            <a href="https://arxiv.org/abs/2412.08467">paper</a> / 
            <a href="https://github.com/wz0919/VLN-SRDF">code</a> /
            <a href="https://github.com/wz0919/VLN-SRDF">bib</a> 
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                  <img src='images/survey.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models</papertitle>
            <br>
            <a href="https://www.egr.msu.edu/~zhan1624/">Yue Zhang<sup>*</sup></a>, <a href="https://mars-tin.github.io/">Ziqiao Ma<sup>*</sup></a>, <strong>Jialu Li</strong><sup>*</sup>, <a href="https://yanyuanqiao.github.io/">Yanyuan Qiao<sup>*</sup></a>, <a href="https://zunwang1.github.io/">Zun Wang<sup>*</sup></a>, <a href="https://web.eecs.umich.edu/~chaijy/">Joyce Chai</a>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>, <a href="https://www.cse.msu.edu/~kordjams/">Parisa Kordjamshidi</a>
            <br>
            <em>Preprint</em>
            <br>
            <a href="https://arxiv.org/abs/2407.07035">paper</a> 
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                  <img src='images/selma.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</papertitle>
            <br>
            <strong>Jialu Li</strong><sup>*</sup>, <a href="https://j-min.io/">Jaemin Cho<sup>*</sup></a>, <a href="https://ylsung.github.io/">Yi-Lin Sung</a>, <a href="https://jaehong31.github.io/">Jaehong Yoon</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>NeurIPS</em>, 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2403.06952">paper</a> /
            <a href="https://github.com/jialuli-luka/SELMA">code</a> /
            <a href="https://github.com/jialuli-luka/SELMA">bib</a> /
            <a href="https://selma-t2i.github.io/">website</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                  <img src='images/vln_video.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VLN-VIDEO: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://aishwaryap.github.io/">Aishwarya Padmakumar</a>, <a href="https://robotics.usc.edu/~gaurav/">Gaurav Sukhatme</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>AAAI</em>, 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2402.03561">paper</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                <img src='images/visual_nav.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Multimodal large language model for visual navigation</papertitle>
            <br>
            <a href="https://yaohungt.github.io/">Yao-Hung Hubert Tsai</a>, <a href="https://openreview.net/profile?id=~Vansh_Dhar2">Vansh Dhar</a>, <a href="https://huguesthomas.github.io/">Hugues Thomas</a>, <strong>Jialu Li</strong>, <a href="https://zbwglory.github.io/">Bowen Zhang</a>, <a href="https://openreview.net/profile?id=~Jian_Zhang23">Jian Zhang</a>
            <br>
            <em>Preprint</em>
            <br>
            <a href="https://arxiv.org/abs/2310.08669">paper</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                <video autoplay controls muted loop width="100%">
                  <source src="images/panorama.mp4" type="video/mp4">
                </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>NeurIPS</em>, 2023</em>
            <br>
            <a href="https://arxiv.org/abs/2305.19195">paper</a> /
            <a href="https://github.com/jialuli-luka/PanoGen">code</a> /
            <a href="https://github.com/jialuli-luka/PanoGen">bib</a> /
            <a href="https://pano-gen.github.io/">website</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
                <img src='images/scalevln.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Scaling Data Generation in Vision-and-Language Navigation</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=G-jPT9MAAAAJ&hl=en">Zun Wang<sup>*</sup></a>, <strong>Jialu Li<sup>*</sup></strong>, <a href="https://yiconghong.me/">Yicong Hong<sup>*</sup></a>, <a href="https://shepnerd.github.io/">Yi Wang</a>, <a href="http://qi-wu.me/">Qi Wu</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>,
            <a href="http://users.cecs.anu.edu.au/~sgould/">Stephen Gould</a>, <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a>, <a href="https://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a>.
            <br>
            <em>ICCV</em>, 2023, </em> <em><span style="color: #e60000;">Oral Presentation</span></em>
            <br>
            <a href="https://arxiv.org/abs/2307.15644">paper</a> /
            <a href="https://github.com/wz0919/scalevln">code</a> /
            <a href="https://github.com/wz0919/scalevln">bib</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/VLN-SIG.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Improving Vision-and-Language Navigation by Generating Future-View Image Semantics</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>CVPR</em>, 2023 </em>
            <br>
            <a href="https://arxiv.org/abs/2304.04907">paper</a> /
            <a href="https://github.com/jialuli-luka/VLN-SIG">code</a> /
            <a href="https://github.com/jialuli-luka/VLN-SIG">bib</a> /
            <a href="https://jialuli-luka.github.io/VLN-SIG">website</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/CLEAR.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment Agnostic Representations</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>Findings of NAACL</em>, 2022 </em>
            <br>
            <a href="https://arxiv.org/abs/2207.02185">paper</a> /
            <a href="https://github.com/jialuli-luka/CLEAR">code</a> /
            <a href="https://github.com/jialuli-luka/CLEAR">bib</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/EnvEdit.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>EnvEdit: Environment Editing for Vision-and-Language Navigation</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>CVPR</em>, 2022 </em>
            <br>
            <a href="https://arxiv.org/abs/2203.15685">paper</a> /
            <a href="https://github.com/jialuli-luka/EnvEdit">code</a> /
            <a href="https://github.com/jialuli-luka/EnvEdit">bib</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/NDH_FULL.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue</papertitle>
            <br>
            <a href="https://hyounghk.github.io/">Hyounghun Kim</a>, <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>EMNLP</em>, 2021 </em>
            <br>
            <a href="https://aclanthology.org/2021.emnlp-main.518/">paper</a> /
            <a href="https://github.com/hyounghk/NDH-FULL">code</a> /
            <a href="https://aclanthology.org/2021.emnlp-main.518.bib">bib</a>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/Syntax_VLN.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</papertitle>
            <br>
            <strong>Jialu Li</strong>, <a href="https://www.cs.unc.edu/~airsplay/">Hao Tan</a>, <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a>.
            <br>
            <em>NAACL</em>, 2021 <em>(short papers)</em>
            <br>
            <a href="https://www.aclweb.org/anthology/2021.naacl-main.82.pdf">paper</a> /
            <a href="https://github.com/jialuli-luka/SyntaxVLN">code</a> /
            <a href="https://www.aclweb.org/anthology/2021.naacl-main.82.bib">bib</a>
          </td>
        </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div>
                <img src='images/Argument_Structure.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Exploring the Role of Argument Structure in Online Debate Persuasion</papertitle>
              <br>
              <strong>Jialu Li</strong>, <a href="https://www.cs.cornell.edu/~esindurmus/">Esin Durmus</a>, <a href="https://www.cs.cornell.edu/home/cardie/">Claire Cardie</a>.
              <br>
              <em>EMNLP</em>, 2020 <em>(short papers)</em>
              <br>
              <a href="https://www.aclweb.org/anthology/2020.emnlp-main.716.pdf">paper</a> /
              <a href="https://github.com/jialuli-luka/Debate_Persuation_Argument_Structure">code</a> /
              <a href="https://www.aclweb.org/anthology/2020.emnlp-main.716.bib">bib</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
                <li> Introduction to Natural Language Processing, Cornell University. Fall 2019. </li>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Service</heading>
              <p>
                <li> Reviewer for ARR, ACL, EMNLP, NAACL, EACL.</li>
                <li> Reviewer for ACM MM, AAAI, CVPR, ICCV, ECCV, ICLR, NeurIPS, WACV, AISTATS.</li>
              </p>
            </td>
          </tr>
        </tbody></table>
      </tbody></table>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href= "https://jonbarron.info/">This guy makes a nice webpage.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
